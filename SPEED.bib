% this file is about DNN speed-up references

% ==== summary
@article{SPEED-arXiv2016-Dumoulin,
    title={A guide to convolution arithmetic for deep learning},
    author={Dumoulin, Vincent and Visin, Francesco},
    journal={arXiv preprint arXiv:1603.07285},
    year={2016}
}
@article{SPEED-arXiv2017-Canziani,
    title   = {An analysis of deep neural network models for practical applications},
    author  = {Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio},
    journal = {arXiv preprint arXiv:1605.07678},
    year    = {2017},
}
@inproceedings{SPEED-DAC2018-Han,
    title     = {Bandwidth-efficient Deep Learning},
    author    = {Han, Song and Dally, William J.},
    booktitle = dac,
    year      = {2018},
    pages     = {147:1--147:6},
}
@article{SPEED-JNeuCom2019-Zhang,
    title     = {Recent advances in convolutional neural network acceleration},
    author    = {Zhang, Qianru and Zhang, Meng and Chen, Tinghuan and Sun, Zhifei and Ma, Yuzhe and Yu, Bei},
    journal   = jneucom,
    volume    = {323},
    pages     = {37--51},
    year      = {2019},
    publisher = {Elsevier},
}
@inproceedings{SPEED-MLSys2020-MNN,
    author = {Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and Lv, Chengfei and Wu, Zhihua},
    title = {{MNN}: A Universal and Efficient Inference Engine},
    booktitle = mlsys,
    year = {2020}
}


% ==================================================================
%                      Architecture Level
% ==================================================================
@inproceedings{SPEED-ISCA2017-Yu,
    title     = {Scalpel: Customizing {DNN} pruning to the underlying hardware parallelism},
    author    = {Yu, Jiecao and Lukefahr, Andrew and Palframan, David and Dasika, Ganesh and Das, Reetuparna and Mahlke, Scott},
    booktitle = isca,
    pages     = {548--560},
    year      = {2017},
}
@inproceedings{SPEED-CVPR2018-MobileNetV2,
    title     = {{MobileNetV2}: Inverted residuals and linear bottlenecks},
    author    = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
    booktitle = cvpr,
    pages     = {4510--4520},
    year      = {2018},
}
@inproceedings{SPEED-CVPR2018-ShuffleNet,
    title     = {{ShuffleNet}: An extremely efficient convolutional neural network for mobile devices},
    author    = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
    booktitle = cvpr,
    pages     = {6848--6856},
    year      = {2018},
}
% ====== group convolution
@inproceedings{SPEED-CVPR2017-ResNeXt,
    title     = {Aggregated residual transformations for deep neural networks},
    author    = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
    booktitle = cvpr,
    pages     = {1492--1500},
    year      = {2017},
}
@inproceedings{SPEED-CVPR2018-CondenseNet,
    title     = {{CondenseNet}: An efficient densenet using learned group convolutions},
    author    = {Huang, Gao and Liu, Shichen and Van der Maaten, Laurens and Weinberger, Kilian Q},
    booktitle = cvpr,
    pages     = {2752--2761},
    year      = {2018},
}
@inproceedings{SPEED-ICCV2019-Zhang,
    title     = {Differentiable Learning-to-Group Channels via Groupable Convolutional Neural Networks},
    author    = {Zhang, Zhaoyang and Li, Jingyu and Shao, Wenqi and Peng, Zhanglin and Zhang, Ruimao and Wang, Xiaogang and Luo, Ping},
    booktitle = iccv,
    pages     = {3542--3551},
    year      = {2019}
}




% ==================================================================
%                      Accurate Speedup 
% ==================================================================
% ==== dense convolution
%{{{
@article{SPEED-arXiv2014-Chetlur,
    title   = {{cuDNN}: Efficient primitives for deep learning},
    author  = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
    journal = arxiv,
    year    = {2014},
}
@inproceedings{SPEED-ICANN2014-Cong,
    title     = {Minimizing computation in convolutional neural networks},
    author    = {Cong, Jason and Xiao, Bingjun},
    booktitle = icann,
    pages     = {281--290},
    year      = {2014},
}
@article{SPEED-arXiv2017-Shi,
    title   = {Speeding up Convolutional Neural Networks By Exploiting the Sparsity of Rectifier Units},
    author  = {Shi, Shaohuai and Chu, Xiaowen},
    journal = arxiv,
    year    = {2017},
}
@inproceedings{SPEED-ICML2017-Cho,
    title     = {{MEC}: memory-efficient convolution for deep neural network},
    author    = {Cho, Minsik and Brand, Daniel},
    booktitle = icml,
    year      = {2017},
}
@inproceedings{SPEED-ASPDAC2017-Mishra,
    title     = {Fine-grained accelerators for sparse machine learning workloads},
    author    = {Mishra, Asit K. and Nurvitadhi, Eriko and Venkatesh, Ganesh and Pearce, Jonathan and Marr, Debbie},
    booktitle = aspdac,
    pages     = {635--640},
    year      = {2017},
}
%}}}
% ==== sparse convolution
%{{{
@inproceedings{SPEED-CVPR2015-Liu,
    title     = {Sparse Convolutional Neural Networks},
    author    = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Penksy, Marianna},
    booktitle = cvpr,
    pages     = {806--814},
    year      = {2015},
}
@inproceedings{SPEED-ICLR2017-Park,
    title     = {Faster {CNNs} with direct sparse convolutions and guided pruning},
    author    = {Park, Jongsoo and Li, Sheng and Wen, Wei and Tang, Ping Tak Peter and Li, Hai and Chen, Yiran and Dubey, Pradeep},
    booktitle = iclr,
    year      = {2017},
    abstract  = {SkimCaffe},
}
% ==== winograd-based
@inproceedings{SPEED-CVPR2016-Lavin,
    title     = {Fast Algorithms for Convolutional Neural Networks},
    author    = {Lavin, Andrew and Gray, Scott},
    booktitle = cvpr,
    pages     = {4013--4021},
    year      = {2016},
}
@inproceedings{SPEED-ICLR2018-Liu,
    title    = {Efficient sparse-winograd convolutional neural networks},
    author   = {Liu, Xingyu and Pool, Jeff and Han, Song and Dally, William J.},
    booktitle= iclr,
    year     = {2018},
}
%}}}
% ==== FFT/DCT based
@inproceedings{SPEED-ICLR015-Vasilache,
    title     = {Fast convolutional nets with fbfft: A {GPU} performance evaluation},
    author    = {Vasilache, Nicolas and Johnson, Jeff and Mathieu, Michael and Chintala, Soumith and Piantino, Serkan and LeCun, Yann},
    booktitle = iclr,
    year      = {2015},
}


% ==================================================================
%                    Pruning 
% ==================================================================
% ==== Sparse
%{{{
@inproceedings{SPEED-NIPS1990-LeCun,
  title     = {Optimal brain damage},
  author    = {LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle = nips,
  pages     = {598--605},
  year      = {1990},
}
@inproceedings{SPEED-NIPS2013-Denil,
  title      = {Predicting parameters in deep learning},
  author     = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and De Freitas, Nando and others},
  booktitle  = nips,
  pages      = {2148--2156},
  year       = {2013}
}
@inproceedings{SPEED-ICML2015-Chen,
    title     = {Compressing neural networks with the hashing trick},
    author    = {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},
    booktitle = icml,
    pages     = {2285--2294},
    year      = {2015},
}
@inproceedings{SPEED-NIPS2015-Han,
    title     = {Learning both weights and connections for efficient neural network},
    author    = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
    booktitle = nips,
    pages     = {1135--1143},
    year      = {2015},
}
@inproceedings{SPEED-ICLR2016-Han,
    title     = {{Deep Compression}: Compressing deep neural networks with pruning, trained quantization and huffman coding},
    author    = {Han, Song and Mao, Huizi and Dally, William J.},
    booktitle = iclr,
    year      = {2016},
}
@inproceedings{SPEED-ICML2015-Chen,
    title     = {Compressing neural networks with the hashing trick},
    author    = {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},
    booktitle = icml,
    pages     = {2285--2294},
    year      = {2015}
}
@inproceedings{SPEED-NIPS2016-Alvarez,
    title     = {Learning the number of neurons in deep networks},
    author    = {Alvarez, Jose M and Salzmann, Mathieu},
    booktitle = nips,
    pages     = {2270--2278},
    year      = {2016}
}
@inproceedings{SPEED-ICLR2017-Ullrich,
    title     = {Soft weight-sharing for neural network compression},
    author    = {Ullrich, Karen and Meeds, Edward and Welling, Max},
    booktitle = iclr,
    year      = {2017},
}
@inproceedings{SPEED-ICLR2017-Li,
    title     = {Pruning filters for efficient convnets},
    author    = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
    booktitle = iclr,
    year      = {2017},
}
@inproceedings{SPEED-ICLR2017-Molchanov,
    title     = {Pruning convolutional neural networks for resource efficient inference},
    author    = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
    booktitle = iclr,
    year      = {2017},
}
@inproceedings{SPEED-ICLR2016-Li,
    title     = {Pruning filters for efficient convnets},
    author    = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
    booktitle = iclr,
    year      = {2016},
}
@inproceedings{SPEED-NIPS2016-Figurnov,
    title     = {PerforatedCNNs: Acceleration through elimination of redundant convolutions},
    author    = {Figurnov, Mikhail and Ibraimova, Aizhan and Vetrov, Dmitry P and Kohli, Pushmeet},
    booktitle = nips,
    year      = {2016},
}
@inproceedings{SPEED-KDD2016-Chen,
    title     = {Compressing Convolutional Neural Networks in the Frequency Domain},
    author    = {Chen, Wenlin and Wilson, James T and Tyree, Stephen and Weinberger, Kilian Q and Chen, Yixin},
    booktitle = kdd,
    pages     = {1475--1484},
    year      = {2016},
}
@inproceedings{SPEED-NIPS2016-Wen,
    title     = {Learning structured sparsity in deep neural networks},
    author    = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
    booktitle = nips,
    pages     = {2074--2082},
    year      = {2016},
}
@inproceedings{SPEED-ECCV2016-Zhou,
  title     = {Less is more: Towards compact cnns},
  author    = {Zhou, Hao and Alvarez, Jose M and Porikli, Fatih},
  booktitle = eccv,
  pages     = {662--677},
  year      = {2016}
}
@inproceedings{SPEED-CVPRW2017-Mao,
    title     = {Exploring the granularity of sparsity in convolutional neural networks},
    author    = {Mao, Huizi and Han, Song and Pool, Jeff and Li, Wenshuo and Liu, Xingyu and Wang, Yu and Dally, William J},
    booktitle = {CVPR Workshop},
    pages     = {13--20},
    year      = {2017},
}
@inproceedings{SPEED-DATE2017-Razlighi,
    title     = {{LookNN}: Neural network with no multiplication}, 
    author    = {M.~S.~Razlighi and M.~Imani and F.~Koushanfar and T.~Rosing}, 
    booktitle = date, 
    year      = {2017}, 
    pages     = {1775-1780}, 
}
@inproceedings{SPEED-ISCA2017-Yu,
    title     = {Scalpel: Customizing dnn pruning to the underlying hardware parallelism},
    author    = {Yu, Jiecao and Lukefahr, Andrew and Palframan, David and Dasika, Ganesh and Das, Reetuparna and Mahlke, Scott},
    booktitle = isca,
    pages     = {548--560},
    year      = {2017},
}
@inproceedings{SPEED-NIPS2017-Aghasi,
    title     = {{Net-Trim}: Convex Pruning of Deep Neural Networks with Performance Guarantee},
    author    = {Aghasi, Alireza and Abdi, Afshin and Nguyen, Nam and Romberg, Justin},
    booktitle = nips,
    pages     = {3180--3189},
    year      = {2017},
}
@inproceedings{SPEED-CVPR2018-Yu,
    title     = {{NISP}: Pruning networks using neuron importance score propagation},
    author    = {Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S},
    booktitle = cvpr,
    year      = {2018},
}
@inproceedings{SPEED-IJCAI2018-Kas,
    title     = {Network Approximation using Tensor Sketching},
    author    = {Kasiviswanathan, Shiva Prasad and Narodytska, Nina and Jin, Hongxia},
    booktitle = ijcai,
    pages     = {2319--2325},
    year      = {2018},
}
@inproceedings{SPEED-ECCV2018-Zhang,
    title     = {A systematic {DNN} weight pruning framework using alternating direction method of multipliers},
    author    = {Zhang, Tianyun and Ye, Shaokai and Zhang, Kaiqi and Tang, Jian and Wen, Wujie and Fardad, Makan and Wang, Yanzhi},
    booktitle = eccv,
    pages     = {184--199},
    year      = {2018}
}
%}}}
% ==== Channel Prunning
@inproceedings{SPEED-ICCV2017-He,
    author    = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
    title     = {Channel Pruning for Accelerating Very Deep Neural Networks},
    booktitle = iccv,
    year      = {2017},
}
@inproceedings{SPEED-ICLR2018-Ye,
    title     = {Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers},
    author    = {Ye, Jianbo and Lu, Xin and Lin, Zhe and Wang, James Z},
    booktitle = iclr,
    year      = {2018}
}
% ==== Low Rank Approximation (LR) 
%{{{
@inproceedings{SPEED-BMVC2014-Jaderberg,
    title     = {Speeding up convolutional neural networks with low rank expansions},
    author    = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
    booktitle = bmvc,
    year      = {2014},
}
@inproceedings{SPEED-NIPS2014-Denton,
    title     = {Exploiting linear structure within convolutional networks for efficient evaluation},
    author    = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
    booktitle = nips,
    pages     = {1269--1277},
    year      = {2014},
}
@inproceedings{SPEED-ICLR2014-Davis,
    title     = {Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks},
    author    = {Davis, Andrew S. and Arel, Itamar},
    booktitle = iclr,
    year      = {2014},
}
@inproceedings{SPEED-ICML2014-Si,
    title     = {Memory efficient kernel approximation},
    author    = {Si, Si and Hsieh, Cho-Jui and Dhillon, Inderjit},
    booktitle = icml,
    pages     = {701--709},
    year      = {2014},
}
@inproceedings{SPEED-ICCV2015-Yang,
    title     = {Deep fried convnets},
    author    = {Yang, Zichao and Moczulski, Marcin and Denil, Misha and de Freitas, Nando and Smola, Alex and Song, Le and Wang, Ziyu},
    booktitle = iccv,
    pages     = {1476--1483},
    year      = {2015},
}
@inproceedings{SPEED-ICLR2015-Lebedev,
    title     = {Speeding-up convolutional neural networks using fine-tuned {CP}-decomposition},
    author    = {Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
    booktitle = iclr,
    year      = {2015},
}
@inproceedings{SPEED-CVPR2015-Zhang,
    title     = {Efficient and accurate approximations of nonlinear convolutional networks}, 
    author    = {Zhang, Xiangyu and Zou, Jianhua and Ming, Xiang and He, Kaiming and Sun, Jian},
    booktitle = cvpr,
    pages     = {1984--1992},
    year      = {2015},
}
@article{SPEED-TPAMI2016-Zhang,
    title     ={Accelerating very deep convolutional networks for classification and detection},
    author    ={Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
    journal   =tpami,
    volume    ={38},
    number    ={10},
    pages     ={1943--1955},
    year      ={2016},
    publisher ={IEEE},
}
@inproceedings{SPEED-ICLR2016-Tai,
    title     = {Convolutional neural networks with low-rank regularization},
    author    = {Tai, Cheng and Xiao, Tong and Zhang, Yi and Wang, Xiaogang and others},
    booktitle = iclr,
    year      = {2016},
}
@inproceedings{SPEED-ICLR2016-Kim,
    title     = {Compression of deep convolutional neural networks for fast and low power mobile applications},
    author    = {Kim, Yong-Deok and Park, Eunhyeok and Yoo, Sungjoo and Choi, Taelim and Yang, Lu and Shin, Dongjun},
    booktitle = iclr,
    year      = {2016},
}
@inproceedings{SPEED-ICCV2017-Wen,
    title     = {Coordinating filters for faster deep neural networks},
    author    = {Wen, Wei and Xu, Cong and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
    booktitle = iccv,
    pages     = {658--666},
    year      = {2017},
}
@inproceedings{SPEED-IJCNN2018-Dai,
    title     = {Fast Training and Model Compression of Gated {RNN}s via Singular Value Decomposition},
    author    = {Dai, Rui and Li, Lefei and Yu, Wenjian},
    booktitle = ijcnn,
    year      = {2018}
}
@inproceedings{SPEED-NIPS2018-GroupReduce,
    title     = {{GroupReduce}: Block-wise low-rank approximation for neural language model shrinking},
    author    = {Chen, Patrick and Si, Si and Li, Yang and Chelba, Ciprian and Hsieh, Cho-Jui},
    booktitle = nips,
    pages     = {10988--10998},
    year      = {2018},
}
%}}}
% ==== Low rank + Sparse
%{{{
@inproceedings{SPEED-CVPR2017-Yu,
    title      = {On compressing deep models by low rank and sparse decomposition},
    author     = {Yu, Xiyu and Liu, Tongliang and Wang, Xinchao and Tao, Dacheng}, 
    booktitle  = cvpr,
    pages      = {7370--7379},
    year       = {2017},
    abstract   = {LRA + sparse},
}
@inproceedings{SPEED-ICCV2017-Liu,
    title     = {Learning efficient convolutional networks through network slimming},
    author    = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
    booktitle = iccv,
    pages     = {2736--2744},
    year      = {2017},
}
@inproceedings{SPEED-NIPS2017-Alvarez,
    title     = {Compression-aware Training of Deep Networks},
    author    = {Alvarez, Jose M. and Salzmann, Mathieu},
    booktitle = nips,
    pages     = {856--867},
    year      = {2017}
}
@inproceedings{SPEED-ICTAI2019-Ma,
    title     = {A Unified Approximation Framework for Non-Linear Deep Neural Networks},
    author    = {Ma, Yuzhe and Chen, Ran and Li, Wei and Shang, Fanhua and Yu, Wenjian and Cho, Minsik and Yu, Bei},
    booktitle = ictai,
    year      = {2019},
}
%}}}


% ==================================================================
%                    Distill Learning 
% ==================================================================
@inproceedings{SPEED-EMNLP2016-Kim,
    title     = {Sequence-level knowledge distillation},
    author    = {Kim, Yoon and Rush, Alexander M},
    booktitle = emnlp,
    pages     = {1317--1327},
    year      = {2016}
}
@inproceedings{SPEED-NIPS2017-Chen,
    title     = {Learning efficient object detection models with knowledge distillation},
    author    = {Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
    booktitle = nips,
    pages     = {742--751},
    year      = {2017},
}


% ==================================================================
%                      Hardware Level
% ==================================================================
% ====== ASIC
@inproceedings{SPEED-ISCA2016-Han,
    title     = {{EIE}: efficient inference engine on compressed deep neural network},
    author    = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
    booktitle = isca,
    pages     = {243--254},
    year      = {2016},
}

