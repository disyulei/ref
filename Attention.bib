% =========================================
%             Attention          
% =========================================
@inproceedings{ATTN-NIPS2014-Mnih,
    title     = {Recurrent models of visual attention},
    author    = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and others},
    booktitle = nips,
    pages     = {2204--2212},
    year      = {2014},
}
@inproceedings{ATTN-NIPS2015-Jaderberg,
    title     = {Spatial transformer networks},
    author    = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and others},
    booktitle = nips,
    pages     = {2017--2025},
    year      = {2015},
}
@inproceedings{ATTN-NIPS2017-Vaswani,
    title     = {Attention is all you need},
    author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    booktitle = nips,
    pages     = {5998--6008},
    year      = {2017},
}
@inproceedings{ATTN-NIPS2015-Chorowski,
    title     = {Attention-based models for speech recognition},
    author    = {Chorowski, Jan K and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
    booktitle = nips,
    pages     = {577--585},
    year      = {2015},
}
@inproceedings{ATTN-NAACL2016-Yang,
    title     = {Hierarchical attention networks for document classification},
    author    = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
    booktitle = naacl,
    pages     = {1480--1489},
    year      = {2016},
}
@inproceedings{ATTN-ACL2017-Cui,
    title     = {Attention-over-attention neural networks for reading comprehension},
    author    = {Cui, Yiming and Chen, Zhipeng and Wei, Si and Wang, Shijin and Liu, Ting and Hu, Guoping},
    booktitle = acl,
    year      = {2017},
}
@inproceedings{ATTN-ECCV2018-CBAM,
    title     = {{CBAM}: Convolutional block attention module},
    author    = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and So Kweon, In},
    booktitle = eccv,
    pages     = {3--19},
    year      = {2018},
}
@inproceedings{ATTN-CVPR2018-SENet,
    title     = {Squeeze-and-excitation networks},
    author    = {Hu, Jie and Shen, Li and Sun, Gang},
    booktitle = cvpr,
    pages     = {7132--7141},
    year      = {2018},
}
@inproceedings{ATTN-CVPR2018-Wang,
    title     = {Non-local neural networks},
    author    = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
    booktitle = cvpr,
    pages     = {7794--7803},
    year      = {2018},
}
@inproceedings{ATTN-NIPS2018-A2-Nets,
    title     = {{A\^{} 2-Nets}: Double Attention Networks},
    author    = {Chen, Yunpeng and Kalantidis, Yannis and Li, Jianshu and Yan, Shuicheng and Feng, Jiashi},
    booktitle = nips,
    pages     = {352--361},
    year      = {2018},
}
@inproceedings{ATTN-ICML2019-LatentGNN,
    title     = {{LatentGNN}: Learning Efficient Non-local Relations for Visual Recognition},
    author    = {Zhang, Songyang and He, Xuming and Yan, Shipeng},
    booktitle = icml,
    pages     = {7374--7383},
    year      = {2019},
}


% ==================================================================
%                    Transformer 
% ==================================================================
@article{ATTN-arXiv2020-Zheng,
    title={{Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers}},
    author={Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and others},
    journal={arXiv preprint arXiv:2012.15840},
    year={2020}
}
@inproceedings{ATTN-ECCV2020-DETR,
    title={End-to-end object detection with transformers},
    author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
    booktitle=eccv,
    pages={213--229},
    year={2020},
}
@article{ATTN-arXiv2020-Dosovitskiy,
    title={An image is worth 16x16 words: {Transformers} for image recognition at scale},
    author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
    journal={arXiv preprint arXiv:2010.11929},
    year={2020}
}

